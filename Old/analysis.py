# -*- coding: utf-8 -*-
"""analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eK66x6oNyAJHcoQCaIcu7ztSBtzBpPCd

# Loading Packages/Functions
"""

import tensorflow as tf
import tensorflow.keras.backend as K

import tensorflow.keras as keras

from tensorflow.keras.models import Sequential,Model,load_model
from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Activation, Bidirectional, Flatten, AdditiveAttention
from tensorflow.keras import utils
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import Sequence

from music21 import *
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import datetime
import pytz
from IPython.display import clear_output

import glob
import pickle




"""# Dataset Analysis

We assume that the data to be analyzed has been extracted in the form of either Music21 streams, or one of the encodings discussed in the report.

## Work with Music21 streams
"""




#separates transposed streams that only have the pitch classes in the C (or a) scale
notes=[]
good=[]
bad=[]
for s in tstreams:
  out=0
  for n in s.flat.notes:
    if isinstance(n,note.Note):
      if n.pitch.pitchClass in [0,2,4,5,7,9,11]:
        notes.append(n.pitch.pitchClass)
      else:
        out=1
  if out==1:
    bad.append(s)
  else:
    good.append(s)
  out=0

notes=[]

for s in tstreams:
  for n in s.flat.notes:
    if isinstance(n,note.Note):
      notes.append(n.pitch.pitchClass)

len(bad)

elements,counts=np.unique(notes,return_counts=True)
notes_hist=dict(zip(elements,counts))

notes_hist

plt.bar(elements,counts)

"""### Time Signatures

Counting the different time signatures for each piece
"""

tstreams[400].getTimeSignatures().show('text')

timesigs=[]
ssstreams=[]
for s in streams:
  temp=s.getTimeSignatures()
  timesigs.append(temp[0].ratioString)
  if temp[0].ratioString=='4/4':
    ssstreams.append(s)

len(ssstreams)

streams[0].getTimeSignatures()[0]

timesigs=[]

for s in ssstreams:
  temp=s.getTimeSignatures()
  timesigs.append(temp[0].ratioString)

len(timesigs)

elements,counts=np.unique(timesigs,return_counts=True)
timesig_hist=dict(zip(elements,counts))

timesig_hist

plt.bar(elements,counts)

"""### Keys

Counting the different Keys
"""

#Give keys written in the stream files (if any)
keys=[]
for s in tstreams:
  k=s.getElementsByClass(key.Key)
  keys.append(k[0].tonicPitchNameWithCase)

ssstreams=[]
for s in sstreams[0:100]:
  temp=stream.Stream(s)
  ssstreams.append(s)

keys=[]

for s in tstreams:
  k = s.analyze('key')
  keys.append(k.tonicPitchNameWithCase)

keyss

elements,counts=np.unique(keys,return_counts=True)
keys_hist=dict(zip(elements,counts))

keys_hist

plt.bar(elements[counts>0],counts[counts>0])

timeSignature = notes_to_parse.getTimeSignatures()[0]
music_analysis = notes_to_parse.analyze('key')
print("Music time signature: {0}/{1}".format(timeSignature.beatCount, timeSignature.denominator))
print("Expected music key: {0}".format(music_analysis))
print("Music key confidence: {0}".format(music_analysis.correlationCoefficient))
print("Other music key alternatives:")
for analysis in music_analysis.alternateInterpretations:
    if (analysis.correlationCoefficient > 0.5):
        print(analysis)

"""### Inter-onset Intervals"""

interonsets=[]
diff=0
for stream in ssstreams:
  for el in stream:
    if isinstance(el,(note.Note,note.Rest)):
      diff=el.offset-diff
      if diff>0:
        interonsets.append(diff)
      diff=el.offset

elements,counts=np.unique(interonsets,return_counts=True)
interonsets_hist=dict(zip(elements,counts))

interonsets_hist

from fractions import Fraction

Fraction(1, 3)

plt.bar(interonsets_hist.keys(),interonsets_hist.values())

def bleh(stream):
  for el in stream:
    if isinstance(el,(note.Note,note.Rest)):
      if el.quarterLength<8:
        return stream

"""### Note Durations"""

durations=[]
big=[]
for stream in sstreams:
  for el in stream:
    if isinstance(el,(note.Note,note.Rest)):
      if el.quarterLength>8:
        big.append(stream)
      durations.append(el.quarterLength)

len(big)

big[10].show('text')

elements,counts=np.unique(durations,return_counts=True)
durations_hist=dict(zip(elements,counts))

1/3 *2

durations_hist

0.125*4

plt.bar(elements[elements<5],counts[elements<5])

"""### Piece Durations"""

piece_durations=[]
small=[]
big=[]
late=[]
total_duration=0
for song in tstreams:
  try:
    s2 = instrument.partitionByInstrument(song) #Change to only grab the piano???
    stream = s2.parts[0].recurse()
  except:
    stream=song.flat
  el=stream[-1]
  try:
    if (el.offset+el.quarterLength)<500:
      small.append(el.offset+el.quarterLength)
    if (el.offset+el.quarterLength)>500:
      big.append(stream)
    piece_durations.append(el.offset+el.quarterLength)
    total_duration+=(el.offset+el.quarterLength)
  except:
    piece_durations.append(el.offset)
    total_duration+=(el.offset)
  if stream[0].offset>0:
      late.append(stream)

len(late)

big[5].show('text')

np.sum(small)/120/60

total_duration/120/60

elements,counts=np.unique(piece_durations,return_counts=True)
piece_durations_hist=dict(zip(elements,counts))

sum(counts)

piece_durations_hist

a=elements[elements<200]
b=a[a>50]
b

plt.bar(elements[elements<200],counts[elements<200])

"""## Work with midi note vectors"""

notes=pd.read_pickle('data/notes_midi')

notes.shape

import matplotlib.pyplot as plt
import matplotlib.lines as mlines

def extract_notes(midi_part):
    parent_element = []
    ret = []
    for nt in midi_part.flat.notes:        
        if isinstance(nt, note.Note):
            ret.append(max(0.0, nt.pitch.ps))
            parent_element.append(nt)
        elif isinstance(nt, chord.Chord):
            for pitch in nt.pitches:
                ret.append(max(0.0, pitch.ps))
                parent_element.append(nt)
    
    return ret, parent_element

def print_parts_countour(midi):
    fig = plt.figure(figsize=(12, 5))
    ax = fig.add_subplot(1, 1, 1)
    minPitch = pitch.Pitch('C10').ps
    maxPitch = 0
    xMax = 0
    
    # Drawing notes.
    for i in range(len(midi.parts)):
        top = midi.parts[i].flat.notes                  
        y, parent_element = extract_notes(top)
        if (len(y) < 1): continue
            
        x = [n.offset for n in parent_element]
        ax.scatter(x, y, alpha=0.6, s=7)
        
        aux = min(y)
        if (aux < minPitch): minPitch = aux
            
        aux = max(y)
        if (aux > maxPitch): maxPitch = aux
            
        aux = max(x)
        if (aux > xMax): xMax = aux
    
    for i in range(1, 10):
        linePitch = pitch.Pitch('C{0}'.format(i)).ps
        if (linePitch > minPitch and linePitch < maxPitch):
            ax.add_line(mlines.Line2D([0, xMax], [linePitch, linePitch], color='red', alpha=0.1))            

    plt.ylabel("Note index (each octave has 12 notes)")
    plt.xlabel("Number of quarter notes (beats)")
    plt.title('Voices motion approximation, each color is a different instrument, red lines show each octave')
    plt.show()

# Focusing only on 6 first measures to make it easier to understand.
print_parts_countour(midi.measures(0, 6))

notes_to_parse.getTimeSignatures()[0]

#load a pickled file containing extracted notes from dataset
object = pd.read_pickle(r'data/notes_hot')
notes_hot=object
n_vocab=128

notes=get_notes_hot()

print(len(notes))
print(notes[0].shape)
print(notes[1].shape)
print(notes[2].shape)

plt.figure(figsize=(16, 6))
plt.imshow(notes_hot[13].T, aspect='auto')
plt.set_cmap('gray_r')
plt.grid(True)

"""Plotting the number of notes in each piece in the dataset. We keep pieces with 50-500 notes (notes or rests)."""

notes_hot=notes_mid



"""# Transpose All Streams"""

tstreams = pd.read_pickle(r'data/transposed_streams_C')

#The following preprocessing step isolates one part of each stream
#Since they are all monophonic, thus having only one part, this serves as to retain only the needed information in a flat structure
sstreams=[]
for midi in streams:
  try: # file has instrument parts
    s2 = instrument.partitionByInstrument(midi) #Change to only grab the piano???
    notes_to_parse = s2.parts[0].recurse() 
    if len(s2.parts)!=1:
      print(notes_to_parse[0])
  except: # file has notes in a flat structure
    notes_to_parse = midi.flat.notes
  sstreams.append(notes_to_parse)

notes=[]
good=[]
bad=[]
for s in tstreams:
  out=0
  for n in s.flat.notes:
    if isinstance(n,note.Note):
      if n.pitch.pitchClass in [0,2,4,5,7,9,11]:
        notes.append(n.pitch.pitchClass)
      else:
        out=1
  if out==1:
    bad.append(s)
  else:
    good.append(s)
  out=0

s=streams[2].flat
k=s.getElementsByClass(key.Key)
k[0]

s=streams[2].flat
k=s.analyze('key')
k

keys=[]
tstreams=[]
for s in streams:
  temp=s.flat
  k=temp.getElementsByClass(key.Key)
  keys.append(k[0])
  iM = interval.Interval(k[0].tonic, pitch.Pitch('C4'))
  im = interval.Interval(k[0].tonic, pitch.Pitch('A3'))
  if k[0].mode=='minor':
    sNew = s.transpose(im)
  else:
    sNew = s.transpose(iM)

  tstreams.append(sNew)

len(tstreams)

import gc
gc.collect()

del streams

for s in tstreams:
  s.transpose(1,inPlace=True)

with open('data/transposed_streams_Dsharp', 'wb') as filepath:
    pickle.dump(tstreams, filepath)

ttstreams=[]
for s in tstreams:
    sNew=s.transpose(1)
    ttstreams.append(sNew)

def transpose_streamsC4_inplace(streams):
  print('First Pass')
  for s in streams:
    k = s.analyze('key')
    print(k.tonicPitchNameWithCase)
    i = interval.Interval(k.tonic, pitch.Pitch('C4'))
    sNew = s.transpose(i,inPlace=True)
  print('Second Pass')
  for s in streams:
    k = s.analyze('key')
    print(k.tonicPitchNameWithCase)
    i = interval.Interval(k.tonic, pitch.Pitch('A3'))
    if k.mode=='minor':
      sNew = s.transpose(i,inPlace=True)

def transpose_sstreamsC4_inplace(sstreams):
  for s in sstreams:
    k=s.getElementsByClass(key.Key)[0]
    iM = interval.Interval(k.tonic, pitch.Pitch('C4'))
    im = interval.Interval(k.tonic, pitch.Pitch('A3'))
    if k.mode=='minor':
      sNew = s.transpose(im,inPlace=True)
    else:
      sNew = s.transpose(iM,inPlace=True)

transpose_sstreamsC4_inplace(sstreams[0:100])

ssstreams=tstreams[0:100]

streams[0].parts[0].recurse().analyze('key')

streams[0].parts[0].show('text')



"""# Gather .krn to one .txt"""

import glob

read_files = glob.glob("/content/gdrive/My Drive/MusicRep/kern_scores/**/*.krn", recursive=True)

with open("data/kern.txt", "wb") as outfile:
    for f in read_files:
        with open(f, "rb") as infile:
            outfile.write(infile.read())

with open("data/kern.txt", "wb") as outfile:
  kern=outfile.read()

